{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PyTorch Cats vs Dogs - Feature Extraction**\n",
    "\n",
    "---\n",
    "![](TransferLearning.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             ReLU-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 4096]      16,781,312\n",
      "             ReLU-37                 [-1, 4096]               0\n",
      "          Dropout-38                 [-1, 4096]               0\n",
      "           Linear-39                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.78\n",
      "Params size (MB): 527.79\n",
      "Estimated Total Size (MB): 747.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchsummary import summary \n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.vgg16(pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model, input_size = (3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Remove the top Dense Fully Connected Layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_classifier = nn.Sequential(*list(model.classifier.children())[:-7])\n",
    "model.classifier = new_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size = (3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Download our data and setup our Transformers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://moderncomputervision.s3.eu-west-2.amazonaws.com/dogs-vs-cats.zip\n",
    "!unzip -q dogs-vs-cats.zip\n",
    "!unzip -q train.zip\n",
    "!unzip -q test1.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory paths for our files\n",
    "train_dir = './Data/train'\n",
    "test_dir = './Data/test1'\n",
    "\n",
    "# Get files in our directories\n",
    "train_files = os.listdir(train_dir)\n",
    "test_files = os.listdir(test_dir)\n",
    "\n",
    "print(f'Number of images in {train_dir} is {len(train_files)}')\n",
    "print(f'Number of images in {test_dir} is {len(test_files)}')\n",
    "\n",
    "transformations = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                      transforms.ToTensor()])\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, filelist, filepath, transform = None):\n",
    "        self.filelist = filelist\n",
    "        self.filepath = filepath\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.filelist))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        imgpath = os.path.join(self.filepath, self.filelist[index])\n",
    "        img = Image.open(imgpath)\n",
    "\n",
    "        if \"dog\" in imgpath:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0 \n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return (img, label)\n",
    "\n",
    "# Create our train and test dataset objects\n",
    "train = Dataset(train_files, train_dir, transformations)\n",
    "val = Dataset(test_files, test_dir, transformations)\n",
    "\n",
    "# Create our dataloaders\n",
    "train_dataset = torch.utils.data.DataLoader(dataset = train, batch_size = 32, shuffle=True)\n",
    "val_dataset = torch.utils.data.DataLoader(dataset = val, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Extract our Features using VGG16**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = os.listdir(\"./Data/train\")\n",
    "image_paths = [\"./Data/train/\"+ x for x in image_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() \n",
    "model = model.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = None\n",
    "    image_labels = None\n",
    "\n",
    "    # loop over each batch and pass our input tensors to hte model\n",
    "    for data, label in tqdm.tqdm(train_dataset):\n",
    "        x = data.cuda()\n",
    "        output = model(x)\n",
    "        \n",
    "        if features is not None:\n",
    "            # Concatenates the given sequence of tensors in the given dimension.\n",
    "            # cat needs at least two tensors so we only start to cat after the first loop\n",
    "            features = torch.cat((features, output), 0)\n",
    "            image_labels = torch.cat((image_labels, label), 0)\n",
    "        else:\n",
    "            features = output\n",
    "            \n",
    "            image_labels = label\n",
    "\n",
    "    # reshape our tensor to 25000 x 25088 \n",
    "    features = features.view(features.size(0), -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we have features for all 25000 images\n",
    "features.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we have labels for all 25000 images\n",
    "image_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape to ensure our features are a flattened 512*7*7 array\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train a LR Classifier using those features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our tensors to numpy arrays\n",
    "features_np = features.cpu().numpy()\n",
    "image_labels_np = image_labels.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split our model into a test and training dataset to train our LR classifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_np, image_labels_np, test_size=0.2, random_state = 7)\n",
    "\n",
    "glm = LogisticRegression(C=0.1)\n",
    "glm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Accruacy\n",
    "accuracy = glm.score(X_test, y_test)\n",
    "print(f'Accuracy on validation set using Logistic Regression: {accuracy*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run some inferences on our Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "imsize = 224\n",
    "\n",
    "loader = transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor()])\n",
    "\n",
    "def image_loader(loader, image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = loader(image).float()\n",
    "    image = torch.tensor(image, requires_grad=True)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test predictions from all models\n",
    "pred_results = test_img()\n",
    "pred_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for i in range(0, 12):\n",
    "    plt.subplot(4, 3, i+1)\n",
    "    result = pred_results[i]\n",
    "    img = test_sample[i]\n",
    "    image = cv2.imread(img)\n",
    "    image = cv2.resize(image, (224, 224), interpolation = cv2.INTER_CUBIC)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.text(72, 248, f'Feature Extractor CNN: {result}', color='lightgreen',fontsize= 12, bbox=dict(facecolor='black', alpha=0.9))\n",
    "    plt.imshow(image)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
